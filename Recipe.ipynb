{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI3DK-EFIVnj"
   },
   "source": [
    "# Recipe1M + Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NgolP8JI30b"
   },
   "source": [
    "In this notebook, we will apply big data algorithms to the Recipe1M+ dataset found [here](http://pic2recipe.csail.mit.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWjQpK8GINUN"
   },
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W0eZy7peJ7s_"
   },
   "outputs": [],
   "source": [
    "# # Java\n",
    "# !apt-get update\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "FiX572qlIKhJ"
   },
   "outputs": [],
   "source": [
    "# Pyhon libary installs\n",
    "# NOTE: Might need apache spark downloads, check later\n",
    "# !pip install pandas\n",
    "\n",
    "# !pip install pyspark\n",
    "# !pip install spark-nlp==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942BjFtDISE-"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "owOzkP2eFw3J"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vector, Vectors, VectorUDT\n",
    "# from org.apache.spark.ml.linal import Vector, Vectors # Typo in John Snow Labs\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *              \n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "K3ygQK3rF0KX"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "twiTmgbWKydQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "_iCIu17IHZRz"
   },
   "outputs": [],
   "source": [
    "# file_path = '/content/drive/MyDrive/layer1.json'\n",
    "file_path = 'layer1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "dXArV2FKSiva"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "# spark = sparknlp.start()\n",
    "# for GPU training >> spark = sparknlp.start(gpu=True)\n",
    "spark = sparknlp.start(gpu=True, memory=\"32G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fT3ENslMSj7i",
    "outputId": "352377b3-2a84-461f-d65b-60351767e2b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1RSD4VXLA1Q"
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can do this with spark later maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop1 = (\"_corrupt_record\", \"url\", \"partition\", \"id\", \"instructions\") #maybe drop some more later\n",
    "df1 = df1.drop(*cols_to_drop1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ingredients: array<struct<text:string>>, title: string]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(df1.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ingredients: array<struct<text:string>>, title: string]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "id": "IsRnSvQ2H7N0"
   },
   "outputs": [],
   "source": [
    "# with open(file_path, 'r') as f:\n",
    "#     pdf = pd.read_json(f)\n",
    "# pdf = pdf.head(100)\n",
    "# print('LENGTH: ', len(pdf)) #1029720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wkO-zsGKYi90",
    "outputId": "ab41f925-e9d3-4f24-d6c2-eb82bbec49b1"
   },
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame(pdf).toDF(*pdf.columns)\n",
    "# cols_to_drop = (\"url\", \"partition\", \"id\", \"instructions\") #maybe drop some more later\n",
    "# df = df.drop(*cols_to_drop)\n",
    "# display(df)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "KfI19Ymeqieb"
   },
   "outputs": [],
   "source": [
    "# # this is definitely new to me\n",
    "# # why to use udf instead of map https://medium.com/@fqaiser94/udfs-vs-map-vs-custom-spark-native-functions-91ab2c154b44\n",
    "# # https://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark\n",
    "\n",
    "# name = 'ingredients' #taret_column name\n",
    "# def parse_ingredients(ingredients):\n",
    "#   # return \"test\"\n",
    "#   return '. '.join([data['text'] for data in ingredients])\n",
    "\n",
    "# udf = UserDefinedFunction(parse_ingredients, StringType())\n",
    "\n",
    "# df = df.select(*[udf(column).alias(name) if column == name else column for column in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is definitely new to me\n",
    "# why to use udf instead of map https://medium.com/@fqaiser94/udfs-vs-map-vs-custom-spark-native-functions-91ab2c154b44\n",
    "# https://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark\n",
    "\n",
    "name = 'ingredients' #taret_column name\n",
    "def parse_ingredients(ingredients):\n",
    "  # return \"test\"\n",
    "  return ', '.join([data['text'] for data in ingredients])\n",
    "\n",
    "udf1 = UserDefinedFunction(parse_ingredients, StringType())\n",
    "\n",
    "df1 = df1.select(*[udf1(column).alias(name) if column == name else column for column in df1.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "h7iNMJWz0sW-"
   },
   "outputs": [],
   "source": [
    "# name = 'instructions' #taret_column name\n",
    "# def parse_instructions(instructions):\n",
    "#   # return \"test\"\n",
    "#   return ' '.join([data['text'] for data in instructions])\n",
    "\n",
    "\n",
    "# udf = UserDefinedFunction(parse_instructions, StringType())\n",
    "\n",
    "# df = df.select(*[udf(column).alias(name) if column == name else column for column in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "uBI6fmWxzkis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         ingredients|               title|\n",
      "+--------------------+--------------------+\n",
      "|6 ounces penne, 2...|Worlds Best Mac a...|\n",
      "|1 c. elbow macaro...|Dilly Macaroni Sa...|\n",
      "|8 tomatoes, quart...|            Gazpacho|\n",
      "|2 12 cups milk, 1...|Crunchy Onion Pot...|\n",
      "|1 (3 ounce) packa...|Cool 'n Easy Crea...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "HV6Pf5ywqioy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1.collect()[0]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "TqNBZrnygG6v"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"ingredients\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "1MPmzapsZR_R"
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer() \\\n",
    "#   .setInputCols(\"document\") \\\n",
    "#   .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "w8Hyl_wxa9K8"
   },
   "outputs": [],
   "source": [
    "# normalizer = Normalizer()\\\n",
    "#   .setInputCols(\"token\") \\\n",
    "#   .setOutputCol(\"normal\")\\\n",
    "#   .setLowercase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_cleaner = StopWordsCleaner()\\\n",
    "#     .setInputCols(\"normal\")\\\n",
    "#     .setOutputCol(\"cleanTokens\")\\\n",
    "#     .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "V9FuVtw0dE2n"
   },
   "outputs": [],
   "source": [
    "# embeddings = BertEmbeddings.pretrained(\"bert_base_uncased\", \"en\") \\\n",
    "#       .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
    "#       .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embeddings = SentenceEmbeddings() \\\n",
    "#         .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "#         .setOutputCol(\"sentence_embeddings\") \\\n",
    "#         .setPoolingStrategy(\"AVERAGE\") \\\n",
    "#         .setLazyAnnotator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L2_768 download started this may take some time.\n",
      "Approximate size to download 139.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Typo in the documentation: BertSentenceEmbeddings, uppercase in the E\n",
    "sentence_embeddings = BertSentenceEmbeddings.pretrained() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setLazyAnnotator(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # took time to find out I needed this finisher\n",
    "embeddings_finisher = EmbeddingsFinisher() \\\n",
    "            .setInputCols([\"sentence_embeddings\"]) \\\n",
    "            .setOutputCols(\"sentence_embeddings_vectors\") \\\n",
    "            .setOutputAsVector(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explodeVectors = SQLTransformer()\\\n",
    "#     .setStatement(\"SELECT EXPLODE(sentence_embeddings_vectors) AS features, * FROM __THIS__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorNormalizer = Normalizer() \\\n",
    "#       .setInputCol(\"features\") \\\n",
    "#       .setOutputCol(\"normFeatures\") \\\n",
    "#       .setP(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[\"sentence_embeddings_vectors\"],\n",
    "#     outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        sentence_embeddings,\n",
    "        embeddings_finisher\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = testPipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+---------------------------+\n",
      "|         ingredients|               title|            document| sentence_embeddings|sentence_embeddings_vectors|\n",
      "+--------------------+--------------------+--------------------+--------------------+---------------------------+\n",
      "|6 ounces penne, 2...|Worlds Best Mac a...|[{document, 0, 51...|[{sentence_embedd...|       [[0.2524833381175...|\n",
      "|1 c. elbow macaro...|Dilly Macaroni Sa...|[{document, 0, 24...|[{sentence_embedd...|       [[0.4110878407955...|\n",
      "|8 tomatoes, quart...|            Gazpacho|[{document, 0, 30...|[{sentence_embedd...|       [[0.2620798349380...|\n",
      "|2 12 cups milk, 1...|Crunchy Onion Pot...|[{document, 0, 17...|[{sentence_embedd...|       [[0.3757590055465...|\n",
      "|1 (3 ounce) packa...|Cool 'n Easy Crea...|[{document, 0, 15...|[{sentence_embedd...|       [[0.4509302973747...|\n",
      "|12 cup shredded c...|Easy Tropical Bee...|[{document, 0, 34...|[{sentence_embedd...|       [[0.4788536727428...|\n",
      "|2 Chicken thighs,...|Kombu Tea Grilled...|[{document, 0, 48...|[{sentence_embedd...|       [[0.3195950984954...|\n",
      "|6 -8 cups fresh r...|Strawberry Rhubar...|[{document, 0, 22...|[{sentence_embedd...|       [[0.3808728456497...|\n",
      "|8 ounces, weight ...|     Yogurt Parfaits|[{document, 0, 12...|[{sentence_embedd...|       [[0.4658582210540...|\n",
      "|2 cups flour, 1 t...|  Zucchini Nut Bread|[{document, 0, 21...|[{sentence_embedd...|       [[0.4263531863689...|\n",
      "|1/2 cup green oni...|Salmon & Salad a ...|[{document, 0, 53...|[{sentence_embedd...|       [[0.4132910370826...|\n",
      "|1 teaspoon fennel...|Fennel-Rubbed Por...|[{document, 0, 31...|[{sentence_embedd...|       [[0.4822209179401...|\n",
      "|1 (750 ml) bottle...|        Pink Sangria|[{document, 0, 23...|[{sentence_embedd...|       [[0.4743337929248...|\n",
      "|14 cup butter, 34...|      Pineapple Loaf|[{document, 0, 23...|[{sentence_embedd...|       [[0.3256974220275...|\n",
      "|200 grams Cake fl...|Brown Sugar 'Kari...|[{document, 0, 27...|[{sentence_embedd...|       [[0.4220629930496...|\n",
      "|1 can tomato sauc...|      Corn Casserole|[{document, 0, 13...|[{sentence_embedd...|       [[0.3813426196575...|\n",
      "|1 12 lbs ground b...|Grandmommy's Mexi...|[{document, 0, 34...|[{sentence_embedd...|       [[0.3078219592571...|\n",
      "|1 (10 ounce) pack...|  Broccoli Rice Bake|[{document, 0, 30...|[{sentence_embedd...|       [[0.3785933256149...|\n",
      "|1/2 cup A.1. Clas...|Steak & Asparagus...|[{document, 0, 20...|[{sentence_embedd...|       [[0.3791988790035...|\n",
      "|1 cup lentils, 12...|Lentils Vegetable...|[{document, 0, 43...|[{sentence_embedd...|       [[0.3707958459854...|\n",
      "+--------------------+--------------------+--------------------+--------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence_embeddings_vectors: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.collect()[0]['sentence_embeddings_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to this sentence embedding to spark ml: https://github.com/JohnSnowLabs/spark-nlp/blob/b95ac300d4fe9b0c6ebdb29ef774d55e672f3067/docs/en/annotators.md#sentenceembeddings\n",
    "def convertToVectorUDF(matrix):\n",
    "  # return \"test\"\n",
    "  return Vectors.dense(matrix.toArray.map(_.cast(DoubleType())))\n",
    "\n",
    "udf_vector = UserDefinedFunction(convertToVectorUDF, DoubleType())\n",
    "\n",
    "# df1 = df1.select(*[udf1(column).alias(name) if column == name else column for column in df1.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: l[0], VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_vectors = test_df.select(list_to_vector_udf(test_df['sentence_embeddings_vectors']).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.25248333811759...|\n",
      "|[0.41108784079551...|\n",
      "|[0.26207983493804...|\n",
      "|[0.37575900554656...|\n",
      "|[0.45093029737472...|\n",
      "|[0.47885367274284...|\n",
      "|[0.31959509849548...|\n",
      "|[0.38087284564971...|\n",
      "|[0.46585822105407...|\n",
      "|[0.42635318636894...|\n",
      "|[0.41329103708267...|\n",
      "|[0.48222091794013...|\n",
      "|[0.47433379292488...|\n",
      "|[0.32569742202758...|\n",
      "|[0.42206299304962...|\n",
      "|[0.38134261965751...|\n",
      "|[0.30782195925712...|\n",
      "|[0.37859332561492...|\n",
      "|[0.37919887900352...|\n",
      "|[0.37079584598541...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is definitely new to me\n",
    "# why to use udf instead of map https://medium.com/@fqaiser94/udfs-vs-map-vs-custom-spark-native-functions-91ab2c154b44\n",
    "# https://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark\n",
    "\n",
    "name = 'ingredients' #taret_column name\n",
    "def parse_ingredients(ingredients):\n",
    "  # return \"test\"\n",
    "  return '. '.join([data['text'] for data in ingredients])\n",
    "\n",
    "udf1 = UserDefinedFunction(parse_ingredients, StringType())\n",
    "\n",
    "df1 = df1.select(*[udf1(column).alias(name) if column == name else column for column in df1.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'sentence_embeddings_vectors' #taret_column name\n",
    "def list_to_vector(vector_list):\n",
    "    return vector_list[0]\n",
    "\n",
    "udf1 = UserDefinedFunction(list_to_vector, )\n",
    "\n",
    "df1 = df1.select(*[udf1(column).alias(name) if column == name else column for column in df1.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.select(explode(\"sentence_embeddings.embeddings\").as(\"sentence_embedding\"))\n",
    "# .withColumn(\"features\", convertToVectorUDF(\"sentence_embedding\")) \n",
    "# Does not work because this is in Scala, not Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.collect()[0]['bert_sentence_embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.DenseVector"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_df.collect()[0]['sentence_embeddings_vectors'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-e19516ff5f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert_sentence_embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvertToVectorUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-e19516ff5f73>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert_sentence_embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvertToVectorUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "name = \"bert_sentence_embeddings\"\n",
    "\n",
    "a = test_df.select(*[convertToVectorUDF(column.embeddings).alias(name) if column == name else column for column in test_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = test_df.select(explode(\"sentence_embeddings.embeddings\").alias(\"sentence_embeddings\")).withColumn(\"features\", convertToVectorUDF(test_df[\"sentence_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = test_df.withColumn(\"features\", convertToVectorUDF(test_df[\"sentence_embeddings\"]))k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = test_df.select(list_to_vector_udf(test_df[\"bert_sentence_embeddings.embeddings\"]).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA_0357877d868a"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "df = spark.createDataFrame([[1, Vectors.dense([0.0, 1.0])],[2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])\n",
    "\n",
    "lda = LDA(k=2, seed=1, optimizer=\"em\")\n",
    "\n",
    "lda.setMaxIter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 1.0])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0]['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0]['features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[\"sentence_embeddings_vectors\"],\n",
    "#     outputCol=\"features\")\n",
    "\n",
    "\n",
    "# output = assembler.transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "lda = LDA(k=num_topics, maxIter=1)\n",
    "model = lda.fit(df_with_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-e93b7235ad16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "num_topics = 3\n",
    "lda = LDA(k=num_topics, maxIter=1)\n",
    "model = lda.fit(c['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumn(\"features\", convertToVectorUDF(\"sentence_embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.collect()[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's explode the sentence_embeddings column and have a new feature column for Spark ML\n",
    "# testPipeline.select(F.explode(\"sentence_embeddings.embeddings\").alias(\"sentence_embedding\"))\n",
    "# .withColumn(\"features\", convertToVectorUDF(\"sentence_embedding\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         ingredients|               title|            document|               token|          embeddings| sentence_embeddings|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|6 ounces penne. 2...|Worlds Best Mac a...|[{document, 0, 51...|[{token, 0, 0, 6,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 c. elbow macaro...|Dilly Macaroni Sa...|[{document, 0, 24...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|8 tomatoes, quart...|            Gazpacho|[{document, 0, 30...|[{token, 0, 0, 8,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|2 12 cups milk. 1...|Crunchy Onion Pot...|[{document, 0, 17...|[{token, 0, 0, 2,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 (3 ounce) packa...|Cool 'n Easy Crea...|[{document, 0, 15...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|12 cup shredded c...|Easy Tropical Bee...|[{document, 0, 34...|[{token, 0, 1, 12...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|2 Chicken thighs....|Kombu Tea Grilled...|[{document, 0, 48...|[{token, 0, 0, 2,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|6 -8 cups fresh r...|Strawberry Rhubar...|[{document, 0, 22...|[{token, 0, 0, 6,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|8 ounces, weight ...|     Yogurt Parfaits|[{document, 0, 12...|[{token, 0, 0, 8,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|2 cups flour. 1 t...|  Zucchini Nut Bread|[{document, 0, 21...|[{token, 0, 0, 2,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1/2 cup green oni...|Salmon & Salad a ...|[{document, 0, 53...|[{token, 0, 2, 1/...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 teaspoon fennel...|Fennel-Rubbed Por...|[{document, 0, 31...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 (750 ml) bottle...|        Pink Sangria|[{document, 0, 23...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|14 cup butter. 34...|      Pineapple Loaf|[{document, 0, 23...|[{token, 0, 1, 14...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|200 grams Cake fl...|Brown Sugar 'Kari...|[{document, 0, 27...|[{token, 0, 2, 20...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 can tomato sauc...|      Corn Casserole|[{document, 0, 13...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 12 lbs ground b...|Grandmommy's Mexi...|[{document, 0, 34...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 (10 ounce) pack...|  Broccoli Rice Bake|[{document, 0, 30...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1/2 cup A.1. Clas...|Steak & Asparagus...|[{document, 0, 20...|[{token, 0, 2, 1/...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "|1 cup lentils. 12...|Lentils Vegetable...|[{document, 0, 43...|[{token, 0, 0, 1,...|[{word_embeddings...|[{sentence_embedd...|[[0.0, 0.0, 0.0, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ingredients: string, title: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, sentence_embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, features: array<array<float>>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features must be of type equal to one of the following types: [struct<type:tinyint,size:int,indices:array<int>,values:array<double>>, array<double>, array<float>] but was actually of type array<array<float>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-af99a01903c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column features must be of type equal to one of the following types: [struct<type:tinyint,size:int,indices:array<int>,values:array<double>>, array<double>, array<float>] but was actually of type array<array<float>>."
     ]
    }
   ],
   "source": [
    "num_topics = 3\n",
    "lda = LDA(k=num_topics, maxIter=1)\n",
    "model = lda.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Py4JError                                 Traceback (most recent call last)\n",
    "# <ipython-input-52-8550de25b96d> in <module>\n",
    "#       1 num_topics = 3\n",
    "#       2 lda = LDA(k=num_topics, maxIter=10)\n",
    "# ----> 3 model = lda.fit(test_df)\n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/base.py in fit(self, dataset, params)\n",
    "#     159                 return self.copy(params)._fit(dataset)\n",
    "#     160             else:\n",
    "# --> 161                 return self._fit(dataset)\n",
    "#     162         else:\n",
    "#     163             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py in _fit(self, dataset)\n",
    "#     333 \n",
    "#     334     def _fit(self, dataset):\n",
    "# --> 335         java_model = self._fit_java(dataset)\n",
    "#     336         model = self._create_model(java_model)\n",
    "#     337         return self._copyValues(model)\n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/ml/wrapper.py in _fit_java(self, dataset)\n",
    "#     330         \"\"\"\n",
    "#     331         self._transfer_params_to_java()\n",
    "# --> 332         return self._java_obj.fit(dataset._jdf)\n",
    "#     333 \n",
    "#     334     def _fit(self, dataset):\n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args)\n",
    "#    1302 \n",
    "#    1303         answer = self.gateway_client.send_command(command)\n",
    "# -> 1304         return_value = get_return_value(\n",
    "#    1305             answer, self.gateway_client, self.target_id, self.name)\n",
    "#    1306 \n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\n",
    "#     109     def deco(*a, **kw):\n",
    "#     110         try:\n",
    "# --> 111             return f(*a, **kw)\n",
    "#     112         except py4j.protocol.Py4JJavaError as e:\n",
    "#     113             converted = convert_exception(e.java_exception)\n",
    "\n",
    "# ~/miniconda3/envs/767_py3/lib/python3.9/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "#     332                     format(target_id, \".\", name, value))\n",
    "#     333         else:\n",
    "# --> 334             raise Py4JError(\n",
    "#     335                 \"An error occurred while calling {0}{1}{2}\".\n",
    "#     336                 format(target_id, \".\", name))\n",
    "\n",
    "# Py4JError: An error occurred while calling o410.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za5PHifHQ9Bb"
   },
   "outputs": [],
   "source": [
    "nlpPipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        stopwords_cleaner,\n",
    "        sentence_embeddings,\n",
    "        embeddings_finisher\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Row(ingredients='6 ounces penne. 2 cups Beechers Flagship Cheese Sauce (recipe follows). 1 ounce Cheddar, grated (1/4 cup). 1 ounce Gruyere cheese, grated (1/4 cup). 1/4 to 1/2 teaspoon chipotle chili powder (see Note). 1/4 cup (1/2 stick) unsalted butter. 1/3 cup all-purpose flour. 3 cups milk. 14 ounces semihard cheese (page 23), grated (about 3 1/2 cups). 2 ounces semisoft cheese (page 23), grated (1/2 cup). 1/2 teaspoon kosher salt. 1/4 to 1/2 teaspoon chipotle chili powder. 1/8 teaspoon garlic powder. (makes about 4 cups)', document=[Row(annotatorType='document', begin=0, end=514, result='6 ounces penne. 2 cups Beechers Flagship Cheese Sauce (recipe follows). 1 ounce Cheddar, grated (1/4 cup). 1 ounce Gruyere cheese, grated (1/4 cup). 1/4 to 1/2 teaspoon chipotle chili powder (see Note). 1/4 cup (1/2 stick) unsalted butter. 1/3 cup all-purpose flour. 3 cups milk. 14 ounces semihard cheese (page 23), grated (about 3 1/2 cups). 2 ounces semisoft cheese (page 23), grated (1/2 cup). 1/2 teaspoon kosher salt. 1/4 to 1/2 teaspoon chipotle chili powder. 1/8 teaspoon garlic powder. (makes about 4 cups)', metadata={'sentence': '0'}, embeddings=[])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Jw0-1EudiAo"
   },
   "outputs": [],
   "source": [
    "pipline_model = nlpPipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lRv4lah-mRD"
   },
   "outputs": [],
   "source": [
    "new_df = pipline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJrhjFGTLSCG"
   },
   "outputs": [],
   "source": [
    "new_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_F0-4bpAVVf"
   },
   "outputs": [],
   "source": [
    "# for row, elem in enumerate(new_df.limit(1).toPandas()['embeddings']):\n",
    "#     print(f'[ROW BEGIN] {row}')\n",
    "#     print((elem[0]))\n",
    "#     print(f'[ROW END] {row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3\n",
    "embeddings_df = new_df.select('title', 'features').limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = embeddings_df.drop('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "model = lda.fit(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/JohnSnowLabs/spark-nlp/issues/2187\n",
    "#  solution to mismatch of types between spark nlp and spark nlp\n",
    "# https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/EmbeddingsFinisherTestSpec.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Recipe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
